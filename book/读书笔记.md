

### Chapter 1 特征工程

#### 1. 为什么需要进行特征归一化 ？  

​        为了消除数据特征之间的量纲影响，使各指标处于同一数量级，以便进行分析。比如在梯度下降过程中：如果没有归一化，那么在差异较大的维度，更新速度较快，在差异较小的维度，更新速度较慢， 如下图左侧；而在归一化到相同数值区间后，各个维度的更新速度变的一致，容易更快的找到梯度下降的最优解， 如下图右侧。

![IMG](placeholder)

##### 相关问题：

#####  常见的归一化有两种：

　（1）线性函数归一化(Min-Max Scaling) 　　　$X_{norm} =\frac{X - X_{min}} {X_{max} - X_{min}}$

​    （2）零均值归一化(Z-Score Normalization)         $z = \frac{X - \mu}{\sigma}$

##### 归一化的适用模型

​        一般来说，通过梯度下降法求解的模型通常是需要归一化的，包括线性回归，逻辑回归， 支持向量机， 神经网络等模型。但是对于决策树模型归一化并不试用， 以C4.5 为例， 决策树在进行节点分裂时主要依据数据集D关于特征x的信息增益比，而信息增益比更特征是否经过归一化是无关的，因为归一化并不会改变样本在特征x上的信息增益。

#### 2. 在进行数据预处理时，　应该怎样处理类别型特征？

**序号编码：**序号编码通常用于处理类别间具有大小关系的数据。如成绩可以分为高中低三档，并且存在“高>中>低” 的排序关系，序号会按照大小关系对类别特征赋予一个数值ID， 例如高表示为３,中表示为２,低表示为１,转换后依然保留了大小关系。

**独热编码 （one-hot encoding）：**　独热编码通常用于处理类别间不具有大小关系的特征。对于类别取值较多的情况使用独热编码要注意两个问题　 (1) 使用稀疏向量来节省空间　（２）配合特征选择来降低维度

[高维度特征会带来几个方面的问题：一是在K近邻算法中，高维空间下两点的距离很难有效衡量；二是在逻辑回归问题中，参数的数量会随着维度的增高而增加，容易引起过拟合问题；三是只有部分维度对分类和预测有帮助]

**二进制编码：**二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别ＩＤ然后将类别ＩＤ对应的二进制编码作为结果。

|  血型  | 类别ID | 二进制表示 |  独热编码   |
| :--: | :--: | :---: | :-----: |
|  A   |  1   | 0 0 1 | 1 0 0 0 |
|  B   |  2   | 0 1 0 | 0 1 0 0 |
|  AB  |  3   | 0 1 1 | 0 0 1 0 |
|  O   |  4   | 1 0 0 | 0 0 0 1 |

*补充：其他的编码方式 Helmert Contrast、Sum Contrast、Ploynomial Contrast、Backward Difference Contrast。*

#### 3. 高维组合特征的处理

为例提高复杂关系的你和能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。以广告点击预估问题为例，原始数据有语言和类型两种离散特征，下表是语言和类型对点击的影响。

| 是否点击 | 语言 |  类型  |
| :------: | :--: | :----: |
|    0     | 中文 |  电影  |
|    1     | 英文 |  电影  |
|    1     | 中文 | 电视剧 |
|    0     | 英文 | 电视剧 |

为了提高拟合能力，语言和类型可以组成二阶特征，下表是语言和类型的组合特征对点击的影响

| 是否<br />点击 | 语言=中文 <br />类型=电影 | 语言=中文        <br />类型=电影 | 语言=中文    <br />类型=电影 | 语言=中文 <br />类型=电影 |
| :------------: | :-----------------------: | :------------------------------: | :--------------------------: | :-----------------------: |
|       0        |             1             |                0                 |              0               |             0             |
|       1        |             0             |                1                 |              0               |             0             |
|       1        |             0             |                0                 |              1               |             0             |
|       0        |             0             |                0                 |              0               |             1             |

​    若一维离散特征参数的规模分别为m和n。那么需要学习的参数的规模为m*n。在互联网环境下，m和n 的数量都可以达到千万量级，几乎无法学习mxn 规模的参数。在这种情况下，一种行之有效的方法是是m和n分别用k维度的低维向量表示。

#### 4. 组合特征

​    基于决策树的特征组合寻找方法：采用梯度提升决策树（每次都在之前构造的决策树的残差上构建下一颗决策树）来构造一棵决策树，从根节点到叶结点的路径都可以看成一种特征组合的方式

#### ５. 文本表示特征（略）

#### ６. Word2Vec（略）

#### 7. 图像数据不足时会带来什么问题？如何缓解数据不足带来的问题？

​    训练数据不足带来的问题主要表现在**过拟合**方面，即模型在训练样本上的效果可能不错，但是在测试上的泛化效果不佳。对应的处理方法大致可以分为两类：

（1）基于模型的方法，主要是**采用降低过拟合风险的措施**，包括简化模型（将非线性模型简化为线性模型）、添加约束项（L1/L2正则项）以缩小假设空间、集成学习、Dropout 超参数等。

（2）基于数据的方法， 主要是通过**数据扩充**，即根据一些先验知识，在保证特定信息的前提下，对原始数据进行适当变换，以达到扩充数据集的效果。

- 具体到图像分类任务，在保持图像类别不变的前提下，可以对训练集中的每幅图像进行以下变换：
  - 一定程度内的随机旋转、平移、缩放、裁剪、填充、左右翻转等。
  - 对图像中的像素添加噪声扰动， 如椒盐噪声、高斯白噪声等。
  - 颜色变换。例如在图像的 $RGB$ 颜色空间进行主成分分析，得到三个主成分的特征向量 $p_1、p_2、p_3$ ，及其对应的特征值 $\lambda1$， $\lambda2$,  $\lambda3$, 然后在每个像素RGB 值上添加增量 $[p_1,p_2,p_3]\cdot[\alpha_1\lambda_1, \alpha_2\lambda_2, \alpha_3\lambda3]^{T}$ ,其中 $\alpha_1, \alpha_2, \alpha_3$ 是均值为0， 方差较小的高斯分布随机数。
  - 改变图像的亮度、清晰度、对比度、锐度等。

- 除了直接在图像空间进行变换， 还可以先对图像进行特征提取， 然后在图像的特征空间内进行变换，利用一些通用的数据扩充或者上采样技术，例如 **SMOTE 算法**。
- 另外，使用生成模型也可以合成一些新样本。例如当今非常流行的对抗生成网络。

​        此外，借助已有的其他模型来进行**迁移学习**也十分常见。比如使用在大规模数据集上预训练好的通用模型，并在针对目标任务的小数据集上进行**微调（fine-tune）**， 这种微调操作就看一看做一种简单的迁移学习。

*补充: SMOTE 算法：Synthetic Monority Over-sampling Technique。*

### Chapter 2 模型评估

#### 1. 准确率和平方根误差的缺陷

##### 准确率

​     **准确率是指分类正确的样本占样本个数的比例**，即 **$Accuracy  = \frac{n_{correct}}{n_{total}}$** ,其中 $n_{correct}$ 为被正确分类的样本个数， $n_{total}$ 为总样本的个数。

​     准确率是分类问题中最简单也是最直观的评价指标， 但存在明显缺陷。当**不同类别的样本比例非常不均衡**时，占比较大的类别往往会成为影响准确率最主要的因素。比如，当负样本占99%时，分类器把所有样本预测为负样本也可以获得99% 的准确率。 

​      为了解决这个问题，可以使用更为有效的**平均准确率**(每个类别下的样本准确率的算术平均)作为评估的指标。

##### 平方根误差 RMSE 

​      RMSE 平方根误差经常被用来衡量回归模型的好坏，其计算公式为 $RMSE = \sqrt{\frac{1}{n}\sum^{n}_{i-1}{(y_{i}-\hat{y_{i}})^2}}$, 其中 $y_i$  是第i个样本点的真实值吗，$\hat{y_i}$  是 第i个样本的预测值，n 是样本点的个数。

​     一般情况下，RMSE 能够更好的反映回归模型预测值与真实值的偏离程度。但在实际问题中，如果存在个别偏离程度非常大的**离群点**(Outlier),   即使离群点数量非常少，也会让RSME 指标变得非常差。

​     针对这种情况从三个角度进行解决。（1） 如何认定这些离群点是“噪声点”的话，就需要在数据预处理的阶段把这些噪声**过滤**掉。（2）如果不认为这些离群点是“噪声点”的话，需要进一步**提高模型的预测能力**，将离群点产生的机制建模进去。（3）找一个更合适的指标来评估模型。比如平均绝对百分比误差**MAPE**， 它定义为  $MAPE  = \sum_{i=1}^{n}\lvert \frac{y_i - \hat{y_i}}{y_i}\rvert \times\frac{100}{n}$,  相比于 RSME， MAPE 相当于把这个点的误差进行了归一化，降低了个别离群点带来的误差影响。

#### 2. 混淆矩阵、P-R曲线、ROC 曲线

![混淆矩阵](./img/matrix.png)

​        **精确率：** 分类正确的正样本个数占分类器判定为正样本的样本个数的比例。即: $Precision = \frac{TP}{TP+FP}$。

​        **召回率：** 分类正确的正样本个数占真正正样本个数的比例。即：$Recall = \frac{TP}{TP+FN}$。

​        Precision 值和 Recall 值是既矛盾又统一的两个指标，为了提高Precision值，分类器需要尽量在”更有把握“时才把样本预测为正样本，但此时由于过于保守而漏掉很多”没有把握“的正样本，导致Recall值降低。

​        P-R 曲线能够综合评估一个排序模型的好坏。P-R 曲线的横轴是召回率，纵轴是精确率。对于一个排序模型来说， 某P-R曲线上的一个点代表着，在某一阈值下，模型将大于该阈值的结果判定为正样本，小于该阈值的结果判定为负样本，此时返回的结果对应的召回率和精确率。整条P-R曲线是通过将阈值从高到底移动而生成的。

​        **假阳性率**：分类错误的正样本(预测为正样本，实际是正样本)占真正负样本的比例。即 $FPR = \frac{FP}{FP+TN}$。

​        **真阳性率**：分类正确的正样本占真正正样本个数的比例。$TPR = \frac{TP}{TP+FN}$。

​        ROC 曲线是通过不断移动分类器区分正负预测结果的阈值来生成曲线上的一组关键点的。从最高的得分开始（实际上是从正无穷开始，对应ROC 曲线的零点）， 逐渐调整到最低得分，每一个阈值都会对应一个FPR和TPR，在ROC图上绘制出每个阈值对应的位置，再连接所有点酒得到最终的ROC 曲线。另外所谓的AUC 是指ROC 曲线下的面积大小，该值能够量化地反应基于ROC 曲线衡量出的模型性能。计算AUC 的值只需要沿着ROC横轴积分即可。由于ROC 曲线一般都处于y=x 这条直线的上方，所以AUC 的取值一般在0.5-1 之间。AUC 越大，说明分类器越可能把真正的正样本排在前面，分类性能越好。

​    相比于 P-R 曲线，ROC 曲线有一个特点，当政府样本的分布发生变化时， ROC 曲线的形状能够基本保持不变，而P-R 曲线的形状一般会发生比较剧烈的变化。所以 ROC 曲线的使用场景更多，被广泛用于排序、推荐、广告等领域。

​        **F1-Score** 也能综合反映一个排序模型的性能。它是精准率和召回率的调和平均值。它的定义为：
$$
F1 = \frac{2*precision*recall}{precision+recall}
$$

#### 3. 余弦距离的应用

​        对于两个向量 A和 B ，其余弦相似度的定义为 $cos(A,B) = \frac{A\cdot B}{\Vert A \Vert_2 \Vert B \Vert_2}$。 即两个向量夹角的余弦，关注的是向量之间的角度关系，并不关心它们的绝对大小，其取值范围是[-1, 1]。 相比较而言，欧式距离的数值受维度的影响，范围不固定，并且含义也比较模糊。总体来说，欧式距离体现在数值上的绝对差异，而余弦距离则体现方向上的相对差异。比如统计两部剧的用户观看行为，用户A 的观看向量是（0，1）， 而用户B 的观看向量为（1，0）；此时两者的余弦距离很大，而欧式距离很小；我们分析两个用户不同视频的偏好，更关注相对差异，显然应当使用余弦距离。而当我们分析用户活跃度时，以登录次数和平均观看市场作为调整，余弦距离会认为（1，10）（10，100） 两个用户距离很近；但是显然两个用户活跃度有着极大差异的，此时我们要关注数值的绝对差异，应当使用欧式距离。

​        考查一个距离是否是严格定义的距离。要从距离的定义出发：在一个集合中，如果每一对元素均可唯一决定一个实数，使得三条距离公理（正定性、对称性、三角不等式）成立，则该实数可称为这对元素之间的距离。以余弦距离为例：

- 正定型：$dist(A, B) = 1-  cos\theta \ge 0 $ 恒成立， 特别地，有  $dist(A, B) = 0 \Leftrightarrow \Vert A \Vert_2 \Vert B \Vert_2 = AB \Leftrightarrow A=B$

  因此余弦距离满足正定性。

- 对称性： 

- $$
  dist(A, B) = 1- cos(A,B) = \frac{\Vert A \Vert_2 \Vert B \Vert_2 - A\cdot B}{\Vert A \Vert_2 \Vert B \Vert_2} = \frac{\Vert B \Vert_2 \Vert A \Vert_2 - A\cdot B}{\Vert A \Vert_2 \Vert B \Vert_2} = dist(B, A)
  $$

  因此，余弦距离满足对称性。

- 三角不等式：该性质并不成立，下面给出一个反例。给定 A = (1, 0)， B = (1, 1)，C=（0，1），则有
  $$
  dist(A, B) = 1 - \frac{\sqrt{2}}{2}
  $$

  $$
  dist(B, C) = 1 - \frac{\sqrt{2}}{2}
  $$

  $$
  dist(A,C) = 1
  $$

  因此有 $dist(A, B)+dist(B,C) = 2-\sqrt{2} < 1 = dist(A, C)$

在机器学习领域，被俗称为距离，却不满足三条距离公理的不仅仅有余弦距离，还有KL距离，也叫相对熵， 它常用于计算两个分布之间的差异，但不满足对称性和三角不等式。

#### 4. 为什么对模型进行过充分的离线评估之后，还要进行在线A/B 测试？如何进行A/B 测试？

需要进行在线 A/B 测试的**原因**如下：

（1）离线评估**无法完全消除模型过拟合的影响**，因此，得出的离线评估结果无法完全替代线上评估结果。

（2）离线评估**无法完全还原线上工程环境**。一般来讲，离线评估往往不会考虑线上环境的延迟、数据丢失、标签数据缺失等情况。因此，离线评估的结果是理想工程环境下的结果。

（3）线上系统的**某些商业指标**在离线评估中无法计算。离线评估一般是针对模型本身进行评估，而与模型相关的其他指标，特别是商业指标，往往无法直接获得。

进行A/B 测试的主要手段是将用户划分为实验组和对照组，对实验组的用户施以新模型，对对照组的用户施以就模型。在划分用户的过程过程中，要注意样本的独立性（同一用户每次只能划分到一个分组中）和采样方式的无偏性（划分过程中选取的usr_id 是一个随机数）。    

####　5. 模型评估中的验证方法

**Holdout检验：**　将原始的样本集合随机划分为训练集和验证集两部分。训练集用于模型训练，　验证集用于模型验证。　**缺点**：在验证集上计算出来的最后评估指标与原始分组有很大关系。

**交叉验证：**　最常见的是ｋ-fold　交叉验证。首先将全部样本划分成ｋ个大小相等的样本子集；依次遍历这ｋ个子集，每次把当前子集作为验证集，其余所有子集作为训练集，进行模型的训练和评估；最后把ｋ次评估指标的平均值作为最终的评估指标。在实际实验中，ｋ经常取10。

**自助法(Bootstrap) : ** 对于总数为ｎ　的样本集合，进行ｎ次有放回的随机采样，得到大小为ｎ的训练集合。ｎ次采样过程中，有的样本会被重复采样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验证。

　　在自助采样过程中。对于样本进行ｎ次自助抽样，当ｎ趋于无穷大时，最终有多少数据从未被选择过？

一个样本在一次抽样过程中未被抽中的概率为　$1 - \frac{1}{n} $，　ｎ次抽样均未被抽中的概率为 $（１-\frac{1}{n})^n$。 当ｎ趋于无穷大时，概率为　
$$
{\lim_{n\to+\infty}}(1-\frac{1}{n})^n 
　 = \lim_{n\to\infty}\frac{1}{(1+\frac{1}{n-1})^n}
　 = \frac{1}{\lim_{n\to\infty}(1+\frac{1}{n-1})^{n-1}} ×\frac{1}{\lim_{n\to\infty}(1+\frac{1}{n-1})}
   = \frac{1}{e} 
   \approx 0.368
$$

####　6. 超参数调优

**网格搜索：**　网格搜索可能是最简单，应用最广泛的超参数搜索算法，它通过查找搜索范围内的所有的点来确定最优值。在实际应用中，我们一般会先使用较广的搜索范围和较大的步长，来寻找全局最优值可能的位置；然后会逐渐缩小搜索范围和步长，来寻找更精确的最优值。

**随机搜索：**　在搜索范围内随机选取样本点而不是测试上界和下界之间之间的所有值。它的理论依据是如果样本点足够大，那么通过随机采样也能大概率找到全局最优值或者近似值、。

**贝叶斯优化算法：**贝叶斯优化算法通过对目标函数的形状进行学习，找到使目标函数向全局最优值提升的参数。具体来说，它的学习目标函数形状的方法是，首先根据先验分布，假设一个搜集函数；然后每一次使用新的采样点来测试目标函数时，利用这个信息来更新目标函数的先验分布；最后，算法测试由后验分布给出的全局最优最可能出现的位置的点。对于贝叶斯算法，有一个需要注意的地方，一旦找到一个局部最优值，它就会在该区域不断采样，所以很容易陷入局部最优值。为了弥补这个缺陷，贝叶斯优化算法会在探索和利用直接找到一个平衡点，“探索”就是在还未取样的区域获取采样点；而“利用”则是根据后验分布在最可能出现的全局区域最值进行采样。

####　7. 在模型评估过程中，过拟合现象和欠拟合现象具体是指什么?

**过拟合**是指模型对于训练数据拟合过当的情况，反应到评估指标上，就是模型在训练集上表现很好，但是在测试集和新数据上表现较差。 **欠拟合**是指模型在训练和预测时表现都不好的情况。

![IMG](./img/underfitting.png)

#### 8. 常见的降低过拟合和欠拟合风险的方法

#####　降低过拟合风险的方法

（１）　获取更多的数据集，或者通过一定的规则来扩充训练数据，　如在图像分类问题上，可以通过平移、旋转、缩放等方法来扩充数据；更进一步地，可以通过对抗生成网络来合成大量的新训练数据。

（２）降低模型的复杂度。当数据较少，模型过于复杂是产生过拟合的主要因素，适当降低模型复杂度可以避免模型拟合过多的采样噪声。例如，在神经网络中减少网络层数、神经元个数等；在决策树模型中降低树的深度、进行剪枝等。

（３）正则化方法。　给模型的参数加上一定的正则约束，比如，将权值的大小加入到损失函数中。以L2正则化为例：$C = C_0 + \frac{\lambda}{2n}\dot\sum_{i}{w_i}^2$。 这样在优化原来目标函数$C_0$　的同时，也能避免权值过大带来的过拟合风险。

（４）集成学习方法。集成学习是把多个模型集成在一起，来降低模型的过拟合风险。

#####　降低欠拟合风险的方法

（１）添加新特征。　当特征不足或者现有特征与样本标签的相关性不强时，　模型容易出现欠拟合。

（２）增加模型复杂度。简单模型的学习能力较差，通过增加模型的复杂度可以使模型拥有较强的拟合能力。例如，在线性模型中添加高次项，在神经网络中添加网络层数或神经元个数等。

（３）减小正则化洗漱。正则化是用来防止过拟合的，但当模型中出现欠拟合现象时，则需要有针对性地减少正则化系数。

*补充：添加特征时，通过挖掘“上下文特征”、“ID 类特征”、“组合特征”等新的特征，往往能取得更好的效果，在深度学习中，有很多模型可以帮助完成特征工程，如因子分解机、梯度提升策略树、Deep-crossing等都可以成为丰富特征的方法*







### Chapter 9 前向神经网络

#### 1. 写出常见的激活函数及其导数

Sigmoid 激活函数的形式为   $$f(z) = \frac{1}{1+e^{-z}}$$,  对应的导函数为    $f'(z) = f(z)(1-f(z))$

Tanh激活函数的形式为   $f(z) = \frac{e^{z} - e^{-z}}{e^{z}+e^{-z}}$,    对应的导函数为$f'(z) = 1 - (f(z))^2$ 

ReLU 激活函数的形式为  $f(z) = max(0, z)$,    对应的导函数为   $$f'(z)=\left\{
\begin{aligned}
1, z > 0 \\
0, z \leq 0 \\
\end{aligned}
\right.$$

#### 2. 为什么 sigmoid 和 tanh 激活函数会导致梯度消失的现象？ 

sigmoid 激活函数的曲线如下左图所示。它将输入z 映射到（0, 1），当z很大时， f(z) 趋近于1；当z很小时， f(z) 趋近于0， 而其导函数 $f'(z) = f(z)(1-f(z)) ​$ 在 z 很大或者很小时都会趋近于0，造成梯度消失的现象。

tanh 激活函数的图像如下右图所示。当z很大时，f(z) 趋近于1， 当z 很小时， z 趋近于-1。 其导数 $f'(z) = 1 - (f(z))^2$  在 z 很大或很小时都会趋近于0，同样会出现梯度消失。

​    实际上，Tanh 相当于 Sigmoid的平移：  $$tanh(z) = 2sigmoid(2x)-1$$。

![sigmoid and tanh](/Users/zhaozhichao/Documents/github/Babel/img/sigmoid_and_tanh.png)

#### 3. ReLU 系列的激活函数相对于 sigmoid 和 tanh 的优点是什么？它们有什么局限性以及如何改进？ 

优点：

（1）从计算角度上， Sigmoid 和 tanh 激活函数均需要计算指数，复杂度高，而ReLU只需要一个阈值即可得到激活值。

（2）ReLU 的非线性包含线性可以有效地解决梯度消失问题，提供相对较宽的激活边界。

（3）ReLU的单侧抑制提供了网络的稀疏表达能力。  

局限性：在训练过程中会会导致神经元死亡的问题。这是由于 $f(z) = max(0, z)$  导致负梯度在经过该ReLU 单元时被置为0， 且在之后也不被任何数据激活，即流经该神经元的梯度永远为0， 不对任何数据产生响应。在实际训练中，如果学习率设置较大，会导致超过一定比例的神经元不可逆死亡，进而参数梯度无法更新，整个训练过程失败。

一些ReLU 的改进措施：

（1）Leaky ReLU（LReLU）

Leaky ReLU 的表示形式为：$$f(z)=\left\{
\begin{aligned}
z, z > 0 \\
\alpha z, z \leq 0 \\
\end{aligned}
\right.$$ ， Leaky ReLU 与 ReLU 的区别在于当 $z\le 0$ 时， 其值不为零，一般来说a 为一个很小的常数(0.01或者0.001数量级的较小整数)，这样既实现了单侧抑制，又保留了部分负梯度信息以致不完全丢失。但另一方面，$ \alpha$  为超参数，较难设置为合适的值，且较为敏感，因此Leaky ReLU 函数在实际使用中的性能部分十分稳定。

（2）参数化 ReLU（Parametric  ReLU，PReLU）：PReLU将负轴部分斜率$ \alpha$ 作为网络中的一个可学习的参数融入模型的整体训练过程。有几点有趣的现象需要注意：

- 自由度较大的各通道独享参数的参数化ReLU性能相比较各通道共享参数更优。

- 在独享参数设定下学到的$ \alpha$取值呈现出由浅层到深层依次递减的趋势，这说明实际上网络所需要的非线性随着网络层数的增加而递减。 

  在分类精度上， 使用PReLU 作为激活函数的网络要优于原始ReLU的网络。但是PReLU在带来更大自由度的同时，也增加了网络模型过拟合的奉献，在实际使用中需要格外注意。

（3）随机化ReLU(Random ReLU, RReLU）：增加了“随机化机制， 其取值在训练阶段服从均匀分布，在测试阶段则将其指定为该均匀分布对应的数学期望 $ \frac{l+u}{2}\ $。

![sigmoid and tanh](/Users/zhaozhichao/Documents/github/Babel/img/relu_adv.png)

（4）指数化线性单元（Exponential Linear Unit， ELU）：2016年 Clevert 等人提出了指数化线性单元 ELU，其公式为： $$ELU(x)=\left\{
\begin{aligned}
x, x \ge 0 \\
\lambda \cdot (e^x - 1), z \lt 0 \\
\end{aligned}
\right.$$。ReLU 具备了 ReLU 函数的优点，同时也解决了ReLU 函数自身的“死区”问题。不过ELU 函数中的指数操作稍稍增大了计算量。在实际应用中，ELU 中的超参数 $\lambda$ 一般被设置为1。

#### 4. 写出多层感知机的平方误差和交叉熵损失函数

给定包含m样本的集合$\{(x^{(1)}, y^{(1)}),...,(x^{(m)}, y^{(m)})\}$, 其整体代价函数为
$$
J(W, b) = [\frac{1}{m}\sum_{i=1}^{m}J({W,b;x^{(i)}, y^{(i)}})] + \frac{\lambda}{2}\sum_{l=1}^{N-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(W_{ij}^{(l)})^2
$$
平方误差损失函数的定义为：
$$
J(W, b) = [\frac{1}{m}\sum_{i=1}^{m}\frac{1}{2}\|y^{(i)}-\mathcal{L}_{W,b}(x^{(i)})|] + \frac{\lambda}{2}\sum_{l=1}^{N-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(W_{ij}^{(l)})^2
$$
其中第一项是平方误差项，第二项为L2正则化项，在功能上可称作权重衰减项，目的是减小权重的幅度，防止过拟合。该项之前的系数$\lambda $ 为权重衰减参数，用于控制损失函数中两项的相对权重。

交叉熵损失函数定义为：
$$
J(W, b) = -[\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{n}y_k^{(i)}lno_k^{(i)}] + \frac{\lambda}{2}\sum_{l=1}^{N-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(W_{ij}^{(l)})^2
$$
其中 $o_k^{(i)}$  代表了第i个样本的预测属于类别k的概率，$y_k^{(i)}$ 为实际的概率（如果第i个样本的真实类别为k，则$y_k^{(i)}=1$, 否则为0） 

#### 5. 反向传播过程以及公式推导







#### 6. 平方误差损失函数和交叉熵损失函数分别适合什么场景？

一般来说，平方损失函数更适合输出为连续，并且最后一层不含 Sigmoid 或者 Softmax 激活函数的神经网络；交叉熵损失更适合二分类或者多分类任务。



#### 7. 神经网络训练时是否可以将全部参数初始化为0？

​    不能。考虑全连接的深度神经网络，同一层的任意神经元都是同构的，它们具有相同的输入和输出，如果再将参数全部初始化为同样的值，那么无论是前向传播还是反向传播的取值都是完全相同的。学习过程将永远无法打破这种对称性，最终同一网络层中的各个参数仍然是相同的。

​    因此，我们需要随机地初始化神经网络参数的值，以打破这种对称性。简单来说，我们可以初始化参数为取值范围$$(-\frac{1}{\sqrt{d}}, \frac{1}{\sqrt{d}})$$ 的均匀分布，其中d是一个神经元接受的输入维度。偏置可以被简单地设为0，并不会导致参数对称的问题。

#### 8. 为什么 Dropout 可以抑制过拟合？

​    Dropout 是指在深度网络的训练中，以一定的概率随机地"临时丢弃"一部分神经元结点。Dropout 的具体实现中，要求每个神经元结点激活值以一定的概率p被丢弃，即该神经元暂时停止工作，因此对包含 N 个神经元结点的网络，在Dropout 的作用下可以看做是 $2^{n}$ 个模型的集成，这 $2^{n}$ 个模型可以认为是原始网络的子网络，它们共享部分权值，并且具有相同的网络层数，而模型整体的参数保持不变，这就大大简化了运算。对于任意神经元，每次训练中都与一组随机挑选的不同的神经元集合共同进行优化，这个过程会减弱全体神经元之间的联合适应性，减少过拟合风险，增强泛化能力。

​    在训练阶段中，每个神经元结点需要增加一个概率系数，被随机丢弃。而在测试阶段，每个神经元的参数要预先乘以概率系数p。   

#### 9. 批量归一化的基本动机与与原理是什么？在卷积神经网络中如何使用？

​    训练数据与测试数据的分布不同将大大降低网络的泛化能力，因此需要在训练开始之前对所有输入数据进行归一化处理。批量归一化方法是针对每一批数据，在网络的每一层输入之前增加归一化处理（均值为0，标准差为1），将所有批数据强制在统一的部分下，即对该层的任意一个神经元（假设为第k维） $\hat{x}^{(k)}$ 采用如下公式：
$$
\hat{x}^{(k)} = \frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}
$$
其中 $x^{(k)}$ 为该层第k个神经元的原始输入数据， $E[x^{(k)}]$ 为这一批数据在第 $k$ 个神经元的均值， $\sqrt{Var[x^{(k)}]}$ 为这一批数据在第k个神经元的标准差。







完整的批量归一化网络层的前向传导过程公式如下：
$$

$$

- [ ]  TBD : BN 

#### 10. 卷积操作的本质特性包括稀疏交互和参数共享，具体解释着两种特性及其作用？

- 稀疏交互(sparse interaction)

  在传统的神经网络中，任意一对输入和输出神经元之间都产生交互，形成稠密的连接结构，如下左图所示。

  而在卷积神经网络中，卷积核尺度远远小于输入的维度，这样每个输出神经元仅与前一层特定局部区域内的神经元存在连接权重，如下右图所示。我们称这种特性为稀疏交互。

  ![sparse_interaction](../img/sparse_interaction.png)

  稀疏交互能够显著减少参数的数量，从而较好的改善过拟合情况。稀疏交互的物理意义在于， 通常图像，文本，语音等现实世界中的数据都具有局部的特征结构，我们可以先学习局部特征，再将局部特征组合起来形成更复杂和抽象的特征。

- 参数共享(parameter sharing)

  在卷积神经网络中，卷积核中的每一个元素都将作用于每一次局部输入的特定位置上。根据参数共享的思想，我们只需要学习一组参数集合，而不需要针对每个位置的每个参数都进行优化，从而大大降低模型的存储需求。
  参数共享的物理意义在于使得参数具有平移等变性。假设图像这种有一只猫，那么无论出现在图像的任何位置，我们都应该将它识别为猫，也就是说神经网络的输出对于平移变换来说应当是等变的。特别地，先将图片进行卷积运算，然后再进行平移和先进行平移，再进行卷积运算两者是相等的。

#### 11. 常用的池化操作有哪些？池化的作用是什么？

常见的池化操作主要针对非重叠区域，包括均值池化(mean pooling)、最大池化(max pooling)等。此外，特殊的池化方式还包括相邻重叠区域的池化(over-lapping)以及空间金字塔池化。

- 均值池化是通过对领域内特征数值求平均来实现，能够抑制由于领域大小受限造成估计值方差增大的现象，特点是对背景的保留效果更好。
- 最大池化则是通过取领域内特征的最大值来来实现，能够抑制网络参数误差造成估计均值偏移的现象，特点是更好地提取纹理信息。
- 重叠区域池化是采用比窗口宽度更小的步长，使得窗口在每次滑动时存在重叠区域。
- 空间金字塔池化主要考虑了多尺度信息的描述，例如同时计算1x1，2x2，4x4 的矩阵的池化并将结果拼接在一起作为下一网络层的输入。

池化的本质是降采样，从而显著降低参数量，此外还能够保持对平移，伸缩，旋转操作的不变性。

#### 12. 卷积神经网络如何用于文本分类任务？（略）

#### 13. 简述ResNet 的核心理论？

​        ResNet 通过调整网络结构来解决梯度消失问题（反向传播时，梯度将涉及多层参数的交叉相乘，可能会在离输入近的网络层中产生梯度消失的现象）。首先考虑两层神经网络的简单叠加，这时 $$x$$ 经过两个网络层的变换得到 $$H(x)$$ ,  激活函数采用  $$ReLU$$， 如下图$$(a)$$所示。既然离输入近的神经网络层较难训练，那么我们可以将它短接到更靠近输出的层，如下图$$(b)$$所示。输入 $$x$$ 经过两个神经网络变换得到 $$F(x)$$, 同时也短接到两层之后，最后这个包含两层的神经网络模块输出 $$H(x) = F(x) + x$$。这样一来， $$F(x)$$ 被设计为只需要拟合输入$$x$$ 与目标输入$$\tilde{H}(x)$$的残差$$\tilde{H}(x)-x​$$ ， 残差网络的名称也因此而来。如果某一层的输出已经较好的拟合了期望结果，那么多加入一层也不会使得模型变得更差，因为该层的输出将直接短接到两层之后，相当于直接学习了一个恒等映射，而跳过的两层只需要拟合上层输出和目标之间的残差即可。

![Resnet](../img/resnet.png)



