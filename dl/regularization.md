### 网络正则化

#### 1. $\mathcal{L_2}$正则化

假设待正则化的网络层参数为 $\omega$, 则 $\mathcal{l_2}$ 正则项形式为：
$$
\mathcal{l}_{2} = \frac{1}{2}\lambda{||\omega||^2_2}
$$
其中， $\lambda$ 控制正则项大小，较大的$\lambda$取值将较大程度地约束模型复杂度；反之亦然。在实际使用时，一般将正则项加入目标函数，通过整体目标函数的误差反向传播，从而达到正则项影响和指导网络训练的目的。$\mathcal{l}_{2}$ 正则化方式在深度学习中有个常用的叫法是“权重衰减”(weight decay), 另外 $\mathcal{l}_2$ 正则化在机器学习中还被称作“岭回归”(ridge regression)  或者 Tikhonov 正则化。

#### 2. $\mathcal{L_1}$ 正则化

$$
\mathcal{l}_1 = \lambda||\omega||_1 = \lambda \sum_i|\omega_i|
$$

需注意，**$l_1$ 正则化除了同 $l_2$正则化一样能约束参数量级外，$l_1$ 正则化还能起到使参数更稀疏的作用**。稀疏化的结果使优化后的参数一部分为 0，另一部分为非0实值。非0实值的那部分参数可起到选择重要参数或特征维度的作用，同时也可以起到去除噪声的作用。此外，  $l_1$ 和 $l_2$ 正则化也可以联合使用，形如：
$$
\lambda_1||\omega||_1 + \lambda_2||\omega||_2^2
$$
这种形式也被称为“Elastic网络正则化”。

#### 3. 最大范数约束

通过向参数量级的范数设置上限对网络进行正则化的手段，形如：
$$
||\omega||_2 < c
$$
其中， $c$ 多取 $10^3$ 或者 $10^4$ 数量级数值。

#### 4. 随机失活 dropout

​        **训练过程中，对于每个 mini-batch, 每个神经元以 $p\%$的概率随机失活，这样每次都对一个更加精简的网络进行训练。而在测试过程中神经元不失活。**

如何理解dropout取得如此好的效果：

**（1）在训练过程中，随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络中的参数。在测试过程中不使用随机失活，可以理解为是对数量巨大的子网络做模型集成(类似于bagging，但是其参数是共享的)。**

**（2）从单个神经元来看，因为每个输入都有可能被随机清除，所以神经元不依赖于某个特定的输入，所以不会在任何的一个输入加上太多的权重。这和L2正则化有异曲同工之妙。**

注意点：

（1）Dropout 是一种正则化的方法，如果模型并没有过拟合，不应该使用dropout。

（2）Dropout中 keep_prob 值的设置问题：经过交叉验证，隐含节点dropout率等于0.5的时候效果最好，dropout也可以被用作一种添加噪声的方法，直接对input进行操作。输入层设为更接近1的数(0.8)。使得输入变化不会太大。对于每一层，可以设置不同的keep_prob, 我们倾向于对结点较多的层使用较低的keep_prob(保留更少节点)。

（3）Dropout 的使用使得无法有效的检验损失函数的下降，此时可以暂时的关闭dropout来检查损失函数值。

#### 5. 验证集的使用

通常，在模型训练前可从训练集数据随机划分出一个子集作为“验证集”。用以在训练阶段评测模型预测性能。一般在每轮或者每批次批处理训练后在该训练集和验证集上分别做网络前向运算，预测训练集和验证集样本标记，绘制学习曲线，以此验证模型的泛化能力。

- 如果验证集的准确率一直低于训练集上的准确率，但无明显下降趋势。这说明模型复杂度欠缺，模型表示能力有限—— 属于欠拟合状态。对此应该通过增加层数、调整激活函数、增加网络非线性、减小模型正则化等措施增大网络复杂度。

- 如果验证集曲线不仅仅低于训练集，且随着训练轮数的增加有明显下降趋势，则说明模型已经过拟合。此时应该增大模型正则化，从而削弱网络复杂度。

#### 6. 其他

除了上述几种网络正则化的方式外，借助验证集**“及时停止”**(early stopping， 也称为“早停”) 网络训练也是一种有效防止网络过拟合的方法。可以取验证集上准确率最高的那一轮作为最终网络，用于测试集数据的预测。

此外，在**数据方面**，可以尝试增加训练数据或者尝试更多数据扩充方式则是一种防止过拟合的方式。
