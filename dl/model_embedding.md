## 集成学习

集成学习（ensemble learning） 是机器学习中的一类学习算法，值训练多个学习器并将它们组合起来使用的方法。一般来讲，深度模型的集成多从“数据层面” 和“模型层面”两个方面着手。

#### 1. 数据层面的集成方法

##### (1) 测试阶段数据扩充

**实际上，训练阶段的数据扩充策略在模型测试阶段同样适用，诸如图像多尺度(multi-scale)、随机抠取(random crop)等**。以随机扣取为例，对某张图像可以得到n张随机扣取图像，测试阶段只需用训练好的深度神经网络对n张图像分别做预测，之后将预测的各类置信度平均作为该测试图像的最终预测结果。

##### (2) 简易集成法(easy ensemble)

**对样本较多的类别采取降采样（under sampling），每次采样数依照样本数目最少的类别而定，这样可以使每类取到的样本数保持均等。采样结束后，针对每次采样得到的子数据集训练模型，如此采样、训练反复进行多次。最后对测试数据的预测则从训练得到的若干个模型的结果取平均或投票获得。**

#### 2. 模型层面的集成方法

##### (1) 单模型集成

**多层模型融合：** **一般地，在进行多层特征融合操作时可以直接将不同层网络特征级联(concatenate)。而对于特征融合应选取哪些网络层，一个实践经验是，最好使用靠近目标函数的几层卷积特征。**

**网络“快照”集成法：** **通过循环调整网络学习率(cyclic learning rate schedule) 可使网络依次收敛到不同的局部最优解处。对于每次循环结束后保存的模型，我们称为“快照”（snapshot）。测试阶段在做模型集成时，一般挑选最后m个模型快照用于集成。**

具体而言，是将网络学习率 $\eta$ 设置为随模型迭代轮数 $t$ 改变的函数， 即：
$$
\eta(t) = \frac{\eta_0}{2}(cos(\frac{\pi mod(t-1,[T/M])}{[T/M]})+1)
$$

其中, $\eta$ 为初始学习率， 一般设置为 0.1 或者 0.2。t 为 模型迭代轮数。T 为模型总的批处理训练次数。M 为学习率"循环退火" 次数。

##### (2) 多模型集成

**多模型生成策略**：可以通过**同一模型不同初始化**、**同一模型不同训练轮数**、**不同目标函数**、**不同网络结构**来生成模型。

**多模型集成方法：**

- **直接平均法**：直接将不同模型产生的类别置信度进行平均得到最后预测结果。   $Final\ score = \frac{1}{N} \sum\limits_i^N{s_i}$

- **加权平均法**：在直接平均法的基础上加入权重来调节不同模型输出的重要程度。
  $$
  Final\ score = \frac{1}{N}\sum_{i=1}^{N}{\omega_is_i}
  $$
  其中， $\omega_i$ 对应第i个模型的权重，且必须满足：$\omega_i \geq 0 $ 且 $\sum\limits_{i=1}^{N}{\omega_i}=1$

- **投票法**：
              投票法中最常用的是**多数表决法**， 表决前需要先将各自模型返回的置信度 $s_i$ 转化为预测类别，即最高置信度对应的类别标记 $c_i \in \{1, 2 , …, C\}$ 作为该模型的预测结果。多数表决法中，在得到样本 $x$ 的最终预测时，若某预测类别获得一半以上模型投票，则该样本预测结果为该类别；若对该样本无任何类别获得一半以上投票，则拒绝做出预测。

  ​        投票法中另一种常用方法是**相对表决法**， 与多数表决法会输出“拒绝预测”不同的是，相对多数表决法一定会返回某个类别作为预测解决，因为相对多数表决是选取投票数最高的类别作为最后预测结果。

- **堆叠法**：

     又称“二次集成法”， 是一种高阶的集成学习算法。样本 $x$ 作为学习算法或者网络模型的输入， $s_i$ 作为第 $i$ 个模型的类别置信度输出， 整个学习过程可记作一阶学习过程。**堆叠法则是一阶学习过程的输出作为输入开展二阶学习过程，有时候也称作“元学习”。 **不过需要支出的是，堆叠法有较大的过拟合风险．

