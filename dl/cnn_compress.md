## 卷积神经网络的压缩

按照压缩过程对网络结构的破坏程度， 我们将模型压缩技术分为"前端压缩"和后端压缩。 

**前端压缩**是指不改变原网络结构的压缩技术， 主要包括**知识蒸馏**、 **紧凑的模型结构设计**以及**滤波器层面的剪枝**。前端压缩未改变原有的网络结构，仅仅是在原有模型的基础上减少网络层数或录波器个数，其最终的模型可完美适配现有的深度学习库。

**后端压缩**则包括**低秩近似**、**未加限制的剪枝**、**参数化**以及**二值网络**。后端压缩未了追求极致的压缩比， 不得不对原有的网络结构进行改造。而这种改造往往是不可逆的，为了获得理想的压缩效果，必须开发相关配套的运行库， 甚至是专门的硬件设备。

在以上的解决方案中， 低秩近似、剪枝与参数量化作为三种常见的压缩技术，已经具备较为明朗的应用前景；其他压缩技术， 如二值网络、知识蒸馏等尚处在发展阶段。

### 知识蒸馏

知识郑亮是迁移学习的一种，其最终目的是阿静一个庞大而复杂的模型所学到的知识，通过一定的技术受到迁移到精简的小模型上， 使得小模型能够获得与大模型相近的性能。**这其中有两个基本要素起着决定性的作用，一是所谓知识， 即如何提取模型中的知识；二十如何蒸馏， 即如何完成知识迁移的任务。**

Jimmy 等人认为， **Softmax层的输入**与类别标签相比，包含了更加丰富的监督信息， 可以被视作网络中的知识的有效概括。

Hinton等人则认为， **Softmax 的输出**会是一种更好的选择， 它包含了每个类别的预测概率， 可以被认为是一种"软标签"。 为了获得更好的"软标签"， 他们使用了一个超参数来控制预测概率的平活程度， 即:
$$
q_i = \frac{exp(z_i / T)}{\sum\limits_j exp(z_j/T)}
$$
其中 T 被称为"温度"， 其值通常为1.

Luo 等人则认为， **可以使用 Softmax 前一层网络的输出**来指导小模型的训练。 这是因为， Softmax 以该层输出为基础进行预测计算， 具有相当的信息量， 却拥有更加紧凑的维度（相对于人脸softmax层的数万维度而言）。

### 剪枝

通过剪枝处理，在减小模型复杂度的同时，还能有效防止过拟合，提升模型泛化性。在神经网络的初始化训练中，我们需要一定冗余度的参数数量来保证模型的可塑性与容量，而在完成训练之后，则可以通过剪枝操作来移除这些冗余
参数，使得模型更加成熟。

操作流程：

1. 衡量神经元的重要程度

2. 移除掉一部分不重要的神经元

3. 对网络进行微调

4. 返回第1步，进行下一轮剪枝

在实际操作中， 还可以借助 $l_1$ 和 $l_2$ 正则化， 以促使网络的权重趋向于零。

- [ ] TBD

### 参数量化

参数量化则是一种常用的后端压缩技术。所谓“量化”，是指从权重中归纳出若干“代表”，由这些“代表”来表示某一类权重的具体数值。

##### 标量量化

最简单也是最基本的一种量化算法便是标量量化。该算法的基本思路是，对于每一个权重矩阵  $W \in \R ^{mn}$，首先将其转化为向量形式: $W \in \R ^{1 \times mn}$ 。之后对该权重向量的元素进行 k 个簇的聚类，这可借助于
经典的 k均值(k)聚类算法快速完成
$$
argmin_c \sum\limits_i^{mn} \sum\limits_j^k ||w_i - c_j||_2^2
$$
如此一来，只需将 k 个聚类中心存储在码本之中便可，而原权重矩阵则只负责记录各自聚类中心在码本中的索引即可。 Han 等人借鉴了网络微调思想，利用后续层的回传梯度对当前的码本进行更新，以降低泛化误差。其具体过程如下图所示：

![parameter_quantization](/Users/zhaozhichao/Desktop/parameter_quantization.png)

##### 乘积量化

- [ ] TBD

### 二值网络

二值网络可以被视为参数量化方法的一种极端情况：所有的参数取值只能是 正负1. 正是这种极端的设定， 使得二值网络能够获得巨大的压缩效益。

- [ ] TBD

### 低秩近似

矩阵操作由矩阵相乘完成， 通常情况下， 权重矩阵稠密且巨大， 解决这个情况最直观的方案是将该稠密矩阵由若干个矩阵近似重构出来， 便能有效降低存储和就按开销。

- [ ] TBD 